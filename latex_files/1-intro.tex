\section{Introduction}
\label{sec:Introduction}
Heart disease remains one of the most significant causes of mortality worldwide, affecting millions of people each year. Early detection and treatment are essential in preventing severe complications, improving life expectancy, and enhancing the quality of life for those at risk. By leveraging data-driven techniques and machine learning algorithms, we can support healthcare professionals by predicting the likelihood of heart disease in patients based on clinical and demographic characteristics, facilitating quicker and more accurate diagnoses.

This project focuses on the Cleveland Heart Disease Dataset, a widely recognized benchmark in medical and machine learning research. This dataset, sourced from the UCI Machine Learning Repository, includes clinical data collected from multiple databases (Cleveland, Hungary, Switzerland, and Long Beach V). However, only the Cleveland subset will be used in this analysis, as it is the most commonly utilized in published studies and provides a reliable basis for heart disease prediction.

The Cleveland subset comprises fourteen selected attributes that reflect relevant demographic and clinical variables, such as age, cholesterol levels, blood pressure, and other factors often associated with heart disease. The target variable, `target`, indicates the presence (1) or absence (0) of heart disease. Our objective is to apply machine learning algorithms to classify patients as either "diseased" or "healthy" based on these attributes. By identifying high-risk individuals more effectively, this project aims to enhance diagnostic processes and enable more targeted treatment strategies.

\section{State of the Art} \label{sec}
\acrfull{ml} has become a fundamental tool in healthcare, especially in disease prediction and diagnosis. By analyzing large volumes of clinical data, \acrshort{ml} models can identify complex patterns that can aid in predicting medical conditions and support decision-making by healthcare professionals. However, the application of \acrshort{ml} in healthcare presents specific challenges, particularly related to model accuracy, interpretability, and reliability. This state of the art section provides an overview of the use of \acrshort{ml} in healthcare, its limitations and challenges, its specific application in the diagnosis of heart diseases, and some of the training approaches and techniques that will be used in this work.

\subsection{Machine Learning in Health Predictions}
In healthcare, machine learning is widely used to create predictive models that assist in the diagnosis and prognosis of diseases, particularly chronic conditions such as diabetes, cancer, and cardiovascular diseases. Predictive models in healthcare need to handle complex data, which may include clinical, demographic, and lifestyle variables of patients. Studies have shown that the use of \acrshort{ml} can improve diagnostic accuracy, but it is essential that these models are interpretable, allowing doctors and specialists to understand the determining factors behind each prediction. However, a recurring problem is class imbalance, where less common diseases are underrepresented in the data, causing the model to be biased toward the dominant class (e.g., healthy patients). In our project, these aspects will be considered when applying \acrshort{ml} for predicting heart diseases.

\subsection{Application of \acrshort{ml} in Heart Disease Prediction}
In the specific context of heart disease prediction, the use of machine learning appears promising in helping identify high-risk patients, enabling preventive interventions. Several studies have already applied \acrshort{ml} algorithms, such as logistic regression, decision trees, and neural networks, to classify the presence of heart disease from clinical data. In our work, we will focus on the \textit{Cleveland Heart Disease Dataset}, a widely used dataset in medical research and ML studies to study and predict the presence of heart disease. This dataset includes variables such as age, blood pressure, cholesterol, and other clinical factors associated with heart risk. Using algorithms such as \textit{Support Vector Classifier (SVC)}, \textit{Logistic Regression}, and \textit{Multi-Layer Perceptron (MLP)}, the goal is to build models that can classify patients as “diseased” or “healthy” with high accuracy.

\subsection{Model Training and Validation Techniques}
To ensure the reliability of the models, it is essential to split the data into training, validation, and test sets. In this project, we will apply the \textit{k-fold cross-validation} technique, which divides the data into multiple training and testing subsets, promoting a more robust evaluation. This method helps prevent overfitting, ensuring that the model generalizes well to new data. During training, the model’s cost function will be monitored throughout the iterations, with graphical visualizations to check the trajectory and convergence of the cost function, both with and without regularization, to better understand the models' behavior.

\subsection{Systematic Hyperparameter Selection}
Choosing the right hyperparameters is a crucial step in developing effective predictive models. Instead of relying on randomly chosen values, this project will adopt a systematic approach to hyperparameter selection, using techniques like \textit{GridSearchCV} and \textit{RandomizedSearchCV}. \textit{GridSearchCV} performs an exhaustive search by testing predefined combinations of hyperparameters in a grid and selecting the best values based on maximizing the model’s accuracy. This approach allows for a comprehensive exploration of different hyperparameter configurations, such as regularization strength and the number of neurons in the MLP’s hidden layer.

On the other hand, \textit{RandomizedSearchCV} offers a more efficient alternative when the search space is very large. Instead of testing all possible combinations, \textit{RandomizedSearchCV} randomly selects a fixed number of hyperparameter combinations from specified distributions, which can significantly reduce computation time while maintaining the chance of finding effective combinations.

These two approaches will be applied to the three selected models (\textit{SVC}, \textit{Logistic Regression}, and \textit{MLP}), aiming to optimize their performance. Combining these hyperparameter selection techniques will provide a robust and efficient evaluation of the models, allowing each algorithm to be tuned to achieve the best possible performance.

\subsection{Model Evaluation: Confusion Matrix and Classification Metrics}
The evaluation of the models will be carried out using the confusion matrix, which provides a detailed view of the model’s correct and incorrect predictions for each class (patients with and without disease). In addition, classification metrics including \textit{accuracy}, \textit{precision}, \textit{recall}, and \textit{F1-score} will be calculated. These metrics are essential for assessing the effectiveness of the model. 

\subsubsection{Recall (Sensitivity)}
Recall is perhaps the most important metric in solving the classification task of this project, that is because it measures the proportion of actual cancer cases that the model correctly identifies. It answers the question: "Out of all patients who truly have cancer, how many did the model detect?". Its formula is the following:
\[Recall=\frac{True Positives (TP)}{True Positives (TP)+False Negatives (FN)}\]
Missing a cancer diagnosis (false negative) can have severe consequences, as early detection is often crucial for effective treatment. A model with low recall might overlook cases of cancer, leading to delayed or missed treatments.
In contrast, a false positive (predicting cancer when it's not present) can cause stress and lead to additional tests, but it is generally less harmful than a false negative.

\subsubsection{Precision}
The precision score is a performance metric used to evaluate the accuracy of a classification model, particularly in binary and multi-class classification problems. It measures the proportion of correctly predicted positive observations to the total predicted positive observations. Precision answers the question: "Of all the instances that were predicted as positive, how many were actually positive?" Its formula is:
\[Precision=\frac{True Positives (TP)}{True Positives (TP)+False Positives (FP)}\]

\subsubsection{F1-Score}
The F1-score, being the harmonic mean (represented below) between precision and recall, will be one of the most important metrics, as it provides a balance between the model’s ability to correctly identify diseased patients and avoid false positives, ensuring that the model is effective both in detecting disease and minimizing diagnostic errors.
\[F1Score=2 \times \frac{Precision*Recall}{Precision+Recall}\]

\subsubsection{Accuracy}
Accuracy is the proportion of correct predictions (both true positives and true negatives) to the total number of predictions made. It represents the overall correctness of the model's predictions. It is especially usefull when the dataset is balanced.

\subsection{Current Challenges and Future Perspectives}
Despite the advancements, the use of machine learning for predicting heart disease faces several challenges. One of the main issues is class imbalance, where the majority of patients in the data belong to the healthy class, which can lead to biased models. Additionally, the interpretability of more complex models, such as neural networks, remains a limitation, as healthcare professionals need to understand the reasoning behind the predictions. One technique that could be used is the application of \textit{deep learning} methods to capture more complex patterns, along with the integration of real-time data, which could enhance model accuracy and enable even more dynamic diagnoses.